# 各种集群配置

# redis

要通过 **Docker Compose** 配置 Redis 集群模式，并让 Spring Boot 项目连接该 Redis 集群，可以按照以下步骤操作：

------

## **1. 配置 Redis 集群模式**

### **1.1 创建 Docker Compose 文件**

以下是一个示例 `docker-compose.yml` 文件，用于启动一个 6 节点的 Redis 集群（3 主 3 从）：

```yaml
version: '3.9'
services:
  redis-node1:
    image: redis:7.0.0
    command: ["redis-server", "/redis-config/redis.conf", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000", "--appendonly", "yes"]
    ports:
      - "7001:6379"
    volumes:
      - ./redis/node1:/redis-config
    networks:
      - redis-cluster

  redis-node2:
    image: redis:7.0.0
    command: ["redis-server", "/redis-config/redis.conf", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000", "--appendonly", "yes"]
    ports:
      - "7002:6379"
    volumes:
      - ./redis/node2:/redis-config
    networks:
      - redis-cluster

  redis-node3:
    image: redis:7.0.0
    command: ["redis-server", "/redis-config/redis.conf", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000", "--appendonly", "yes"]
    ports:
      - "7003:6379"
    volumes:
      - ./redis/node3:/redis-config
    networks:
      - redis-cluster

  redis-node4:
    image: redis:7.0.0
    command: ["redis-server", "/redis-config/redis.conf", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000", "--appendonly", "yes"]
    ports:
      - "7004:6379"
    volumes:
      - ./redis/node4:/redis-config
    networks:
      - redis-cluster

  redis-node5:
    image: redis:7.0.0
    command: ["redis-server", "/redis-config/redis.conf", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000", "--appendonly", "yes"]
    ports:
      - "7005:6379"
    volumes:
      - ./redis/node5:/redis-config
    networks:
      - redis-cluster

  redis-node6:
    image: redis:7.0.0
    command: ["redis-server", "/redis-config/redis.conf", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000", "--appendonly", "yes"]
    ports:
      - "7006:6379"
    volumes:
      - ./redis/node6:/redis-config
    networks:
      - redis-cluster

networks:
  redis-cluster:
    driver: bridge
```

### **1.2 创建 Redis 配置文件**

为每个节点（`node1`, `node2`...）创建 Redis 配置文件，例如：

#### 文件路径：`./redis/node1/redis.conf`

```conf
port 6379
bind 0.0.0.0
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes  #开启aof数据存储
```

- 重复为 `node2` 到 `node6` 创建类似的配置文件。

- 两种数据持久化的方式，在配置文件中添加如下内容：

  ```conf
  #RDB方式
  save 900 1      # 900秒内至少1个键被修改则触发保存
  save 300 10     # 300秒内至少10个键被修改则触发保存
  save 60 10000   # 60秒内至少10000个键被修改则触发保存
  dbfilename dump.rdb  # RDB文件名
  dir ./  # RDB文件存储目录
  
  #AOF方式，更加具有实时性
  appendonly yes  # 开启AOF持久化
  appendfilename "appendonly.aof"  # AOF文件名
  appendfsync everysec  # 每秒同步一次至磁盘
  
  ```

  

### **1.3 启动 Redis 集群**

运行以下命令启动集群：

```bash
docker-compose up -d
```

------

### **1.4 初始化 Redis 集群**

Redis 集群需要手动初始化，运行以下命令将节点组合成集群：

1. 进入任意一个 Redis 节点的容器：

   ```bash
   docker exec -it redis-node1 redis-cli
   ```

2. 初始化集群（假设所有节点都运行在同一宿主机的不同端口）：

   ```bash
   redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 --cluster-replicas 1
   ```

   - `--cluster-replicas 1` 表示每个主节点有一个从节点。

3. 在容器内查看集群状态：

   ```bash
   redis-cli -c -p 7001 cluster nodes
   ```

------

## **2. Spring Boot 连接 Redis 集群**

### **2.1 添加依赖**

在 `pom.xml` 文件中添加 Redis 的依赖：

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

------

### **2.2 配置 Redis 集群连接**

在 `application.yml` 或 `application.properties` 中添加 Redis 集群的配置：

#### **application.yml**

```yaml
spring:
  redis:
    cluster:
      nodes:
        - 127.0.0.1:7001
        - 127.0.0.1:7002
        - 127.0.0.1:7003
        - 127.0.0.1:7004
        - 127.0.0.1:7005
        - 127.0.0.1:7006
      max-redirects: 3
```

------

### **2.3 配置 RedisTemplate（可选）**

在 Spring Boot 中，可以通过 `RedisTemplate` 来进行 Redis 操作。以下是一个自定义的配置：

#### **RedisConfig.java**

```java
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisClusterConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.StringRedisSerializer;

import java.util.Arrays;

@Configuration
public class RedisConfig {

    @Bean
    public LettuceConnectionFactory redisConnectionFactory() {
        // 配置集群节点,如果需要自定义连接池的大小可以配置这个Bean
        RedisClusterConfiguration redisClusterConfiguration = new RedisClusterConfiguration(
            Arrays.asList("127.0.0.1:7001", "127.0.0.1:7002", "127.0.0.1:7003", 
                          "127.0.0.1:7004", "127.0.0.1:7005", "127.0.0.1:7006"));
      GenericObjectPoolConfig<Object> poolConfig = new GenericObjectPoolConfig<>();
    poolConfig.setMaxTotal(50);
    poolConfig.setMinIdle(5);
    poolConfig.setMaxIdle(10);

    return new LettuceConnectionFactory(clusterConfiguration, poolConfig);
    }

    @Bean
    public RedisTemplate<String, Object> redisTemplate(LettuceConnectionFactory redisConnectionFactory) {
        RedisTemplate<String, Object> redisTemplate = new RedisTemplate<>();
        redisTemplate.setConnectionFactory(redisConnectionFactory);

        // 设置序列化器
        redisTemplate.setKeySerializer(new StringRedisSerializer());
        redisTemplate.setValueSerializer(new StringRedisSerializer());
        redisTemplate.setHashKeySerializer(new StringRedisSerializer());
        redisTemplate.setHashValueSerializer(new StringRedisSerializer());

        return redisTemplate;
    }
}
```

------

## **3. 验证 Redis 集群连接**

1. 在代码中使用 `RedisTemplate` 来操作 Redis 集群，例如：

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class RedisController {

    @Autowired
    private RedisTemplate<String, Object> redisTemplate;

    @GetMapping("/set")
    public String setKey() {
        redisTemplate.opsForValue().set("key", "value");
        return "Key set";
    }

    @GetMapping("/get")
    public String getKey() {
        return (String) redisTemplate.opsForValue().get("key");
    }
}
```

1. 运行 Spring Boot 项目，并通过浏览器或 Postman 测试 `/set` 和 `/get` 接口，验证数据是否正确存储和读取。

在 Redis 集群模式下，数据的存储遵循 Redis 集群的分片规则，数据被分布式存储在不同的主节点上。

#### **3.1 Redis 集群的存储机制**

1. **哈希槽（Hash Slot）机制**：
   - Redis 集群将键空间分为 16384 个哈希槽（Hash Slot）。
   - 每个键通过 CRC16 算法计算哈希值，并对 16384 取模，得到该键所属的哈希槽编号。
   - 集群的主节点按槽分布数据，每个主节点负责一部分哈希槽范围。
2. **主从复制**：
   - 集群中的每个主节点通常有一个或多个从节点，主节点负责写入，从节点负责同步主节点数据，用于高可用。

#### **3.2 Spring Boot 中存储数据的过程**

1. **Redis 客户端逻辑**：

   - Spring Boot 使用 `spring-data-redis` 和 `Lettuce` 或 `Jedis` 客户端。
   - 客户端根据键的哈希槽，将数据直接发送到对应的主节点。
   - 示例流程：
     - `key1` 的哈希槽编号为 4000，存储在 `Node 1`。
     - `key2` 的哈希槽编号为 6000，存储在 `Node 2`。
     - `key3` 的哈希槽编号为 12000，存储在 `Node 3`。

2. **代码示例**：

   ```java
   java
   
   
   复制代码
   @Autowired
   private RedisTemplate<String, String> redisTemplate;
   
   public void storeData() {
       redisTemplate.opsForValue().set("key1", "value1");  // 存储到 Node 1
       redisTemplate.opsForValue().set("key2", "value2");  // 存储到 Node 2
       redisTemplate.opsForValue().set("key3", "value3");  // 存储到 Node 3
   }
   ```

3. **键标签（Hash Tag）支持**：

   - 如果希望一组键存储在同一个节点上，可以使用 Redis 的键标签（hash tag）。

   - 键标签格式：在键中包含 `{...}`，Redis 集群只对 `{}` 中的内容进行哈希计算。

   - 示例：

     ```java
     java
     
     
     复制代码
     redisTemplate.opsForValue().set("user:{123}", "data1");  // 与 user:{456} 存在同一节点
     redisTemplate.opsForValue().set("user:{456}", "data2");
     ```

#### **3.3 总结存储方式**

- **分布式存储**：
  - 数据会被分片存储到不同的主节点上。
  - 主节点负责写入，从节点负责同步。
- **高可用保障**：
  - 如果某个主节点故障，Redis 集群会自动提升从节点为主节点，继续提供服务。

------

## **4. 总结**

- 使用 Docker Compose 启动 Redis 集群。
- 使用 `redis-cli` 初始化 Redis 集群。
- 在 Spring Boot 项目中，通过 `application.yml` 配置集群节点。
- 使用 `RedisTemplate` 操作 Redis 集群。
- 可以结合 Redis 集群模式实现高可用和分布式存储，适合生产环境中的大规模数据存储和高并发场景。
- ==redis本身不具备分布式集群，需要进入容器使用命令进行集群设置，springboot连接时也需要指定所有节点，但是比MySQL又好一点，不需要中间服务，如shardingsphere或mycat==

# MySQL

以下是使用 Docker 配置 **MySQL 主从集群模型** 的完整教程，包括 **一主一从** 和 **双主双从** 模式的设置。

------

## **1. 一主一从模式**

### **1.1 Docker Compose 配置**

#### **docker-compose.yml**

以下是一个简单的 MySQL 一主一从配置文件：

```yaml
version: '3.9'
services:
  mysql-master:
    image: mysql:8.0
    container_name: mysql-master
    environment:
      MYSQL_ROOT_PASSWORD: root_password
      MYSQL_DATABASE: mydb
    ports:
      - "3306:3306"
    volumes:
      - ./master-data:/var/lib/mysql
      - ./master-config:/etc/mysql/conf.d
    networks:
      - mysql-cluster

  mysql-slave:
    image: mysql:8.0
    container_name: mysql-slave
    environment:
      MYSQL_ROOT_PASSWORD: root_password
    ports:
      - "3307:3306"
    volumes:
      - ./slave-data:/var/lib/mysql
      - ./slave-config:/etc/mysql/conf.d
    networks:
      - mysql-cluster

networks:
  mysql-cluster:
    driver: bridge
```

------

### **1.2 配置 MySQL Master**

1. 在 `./master-config` 目录下创建 `master.cnf` 文件：

   ```ini
   [mysqld]
   server-id=1
   log_bin=mysql-bin
   binlog_format=row
   ```

2. 启动 Docker Compose：

   ```bash
   docker-compose up -d
   ```

3. 进入 `mysql-master` 容器并创建一个复制用户：

   ```bash
   docker exec -it mysql-master bash
   mysql -u root -p
   ```

   执行以下 SQL 命令：

   ```sql
   CREATE USER 'replica_user'@'%' IDENTIFIED WITH 'mysql_native_password' BY 'replica_password';
   GRANT REPLICATION SLAVE ON *.* TO 'replica_user'@'%';
   FLUSH PRIVILEGES;
   SHOW MASTER STATUS;
   ```

   **记住输出的 `File` 和 `Position` 字段值（例如：`mysql-bin.000001` 和 `154`），稍后配置从节点会用到。**

------

### **1.3 配置 MySQL Slave**

1. 在 `./slave-config` 目录下创建 `slave.cnf` 文件：

   ```ini
   [mysqld]
   server-id=2
   relay_log=relay-bin
   ```

2. 进入 `mysql-slave` 容器并配置复制：

   ```bash
   docker exec -it mysql-slave bash
   mysql -u root -p
   ```

   执行以下 SQL 命令（替换 `MASTER_LOG_FILE` 和 `MASTER_LOG_POS` 为主节点的值）：

   ```sql
   CHANGE MASTER TO 
     MASTER_HOST='mysql-master',
     MASTER_USER='replica_user',
     MASTER_PASSWORD='replica_password',
     MASTER_LOG_FILE='mysql-bin.000001',
     MASTER_LOG_POS=154;
   START SLAVE;
   SHOW SLAVE STATUS\G;
   ```

3. 确认主从复制成功：

   - 在从节点运行 `SHOW SLAVE STATUS\G`，确认 `Slave_IO_Running` 和 `Slave_SQL_Running` 都是 `Yes`。

------

## **2. 双主双从模式**

在双主双从模式中，两个主节点互为主从，同时每个主节点有一个从节点。这种架构提供更高的可用性和扩展性。

------

### **2.1 Docker Compose 配置**

#### **docker-compose.yml**

以下是一个 Docker Compose 文件，配置双主双从模式：

```yaml
version: '3.9'
services:
  mysql-master1:
    image: mysql:8.0
    container_name: mysql-master1
    environment:
      MYSQL_ROOT_PASSWORD: root_password
      MYSQL_DATABASE: mydb
    ports:
      - "3306:3306"
    volumes:
      - ./master1-data:/var/lib/mysql
      - ./master1-config:/etc/mysql/conf.d
    networks:
      - mysql-cluster

  mysql-master2:
    image: mysql:8.0
    container_name: mysql-master2
    environment:
      MYSQL_ROOT_PASSWORD: root_password
    ports:
      - "3307:3306"
    volumes:
      - ./master2-data:/var/lib/mysql
      - ./master2-config:/etc/mysql/conf.d
    networks:
      - mysql-cluster

  mysql-slave1:
    image: mysql:8.0
    container_name: mysql-slave1
    environment:
      MYSQL_ROOT_PASSWORD: root_password
    ports:
      - "3308:3306"
    volumes:
      - ./slave1-data:/var/lib/mysql
      - ./slave1-config:/etc/mysql/conf.d
    networks:
      - mysql-cluster

  mysql-slave2:
    image: mysql:8.0
    container_name: mysql-slave2
    environment:
      MYSQL_ROOT_PASSWORD: root_password
    ports:
      - "3309:3306"
    volumes:
      - ./slave2-data:/var/lib/mysql
      - ./slave2-config:/etc/mysql/conf.d
    networks:
      - mysql-cluster

networks:
  mysql-cluster:
    driver: bridge
```

------

### **2.2 配置 Master 1 和 Master 2**

1. 在 `./master1-config/master.cnf` 文件中：

   ```ini
   [mysqld]
   server-id=1
   log_bin=mysql-bin
   binlog_format=row
   log-slave-updates            # 确保从主节点同步的数据也写入自己的二进制日志
   ```

2. 在 `./master2-config/master.cnf` 文件中：

   ```ini
   [mysqld]
   server-id=2
   log_bin=mysql-bin
   binlog_format=row
   log-slave-updates            # 确保从主节点同步的数据也写入自己的二进制日志
   ```

3. 启动 Docker Compose：

   ```bash
   docker-compose up -d
   ```

4. **配置主从关系（双向同步）**：

   - 进入 `mysql-master1`，创建复制用户并获取状态：

     ```bash
     docker exec -it mysql-master1 bash
     mysql -u root -p
     ```

     执行以下 SQL：

     ```sql
     CREATE USER 'replica_user'@'%' IDENTIFIED WITH 'mysql_native_password' BY 'replica_password';
     GRANT REPLICATION SLAVE ON *.* TO 'replica_user'@'%';
     FLUSH PRIVILEGES;
     SHOW MASTER STATUS;
     ```

     记下 `File` 和 `Position`。

   - 进入 `mysql-master2`，创建相同的复制用户：

     ```bash
     docker exec -it mysql-master2 bash
     mysql -u root -p
     ```

     执行以下 SQL：

     ```sql
     CREATE USER 'replica_user'@'%' IDENTIFIED WITH 'mysql_native_password' BY 'replica_password';
     GRANT REPLICATION SLAVE ON *.* TO 'replica_user'@'%';
     FLUSH PRIVILEGES;
     SHOW MASTER STATUS;
     ```

     记下 `File` 和 `Position`。

5. **设置双向复制**：

   - 在 `mysql-master1` 配置主从关系：

     ```sql
     CHANGE MASTER TO 
       MASTER_HOST='mysql-master2',
       MASTER_USER='replica_user',
       MASTER_PASSWORD='replica_password',
       MASTER_LOG_FILE='mysql-bin.000001',
       MASTER_LOG_POS=154;
     START SLAVE;
     ```

   - 在 `mysql-master2` 配置主从关系：

     ```sql
     CHANGE MASTER TO 
       MASTER_HOST='mysql-master1',
       MASTER_USER='replica_user',
       MASTER_PASSWORD='replica_password',
       MASTER_LOG_FILE='mysql-bin.000001',
       MASTER_LOG_POS=154;
     START SLAVE;
     ```

------

### **2.3 配置 Slave 1 和 Slave 2**

1. 为 Slave 1 配置：

   - 在 `./slave1-config/slave.cnf` 文件中：

     ```ini
     [mysqld]
     server-id=3
     relay_log=relay-bin
     ```

   - 在 `mysql-slave1` 中设置主节点为 `mysql-master1`：

     ```sql
     CHANGE MASTER TO 
       MASTER_HOST='mysql-master1',
       MASTER_USER='replica_user',
       MASTER_PASSWORD='replica_password',
       MASTER_LOG_FILE='mysql-bin.000001',
       MASTER_LOG_POS=154;
     START SLAVE;
     ```

2. 为 Slave 2 配置：

   - 在 `./slave2-config/slave.cnf` 文件中：

     ```ini
     [mysqld]
     server-id=4
     relay_log=relay-bin
     ```

   - 在 `mysql-slave2` 中设置主节点为 `mysql-master2`：

     ```sql
     CHANGE MASTER TO 
       MASTER_HOST='mysql-master2',
       MASTER_USER='replica_user',
       MASTER_PASSWORD='replica_password',
       MASTER_LOG_FILE='mysql-bin.000001',
       MASTER_LOG_POS=154;
     START SLAVE;
     ```

------

### **3. 测试双主双从模式**

1. 在 `mysql-master1` 中插入数据：

   ```sql
   USE mydb;
   INSERT INTO test_table (name) VALUES ('test');
   ```

2. 数据会同步到 `mysql-master2` 和所有从节点。

3. 在从节点中验证：

   ```sql
   SELECT * FROM test_table;
   ```

------

### **总结**

1. **一主一从模式**：
   - 设置一个主节点，一个从节点，主节点处理写请求，从节点同步数据。
2. **双主双从模式**：
   - 两个主节点互为主从，每个主节点有一个从节点，形成高可用的集群架构。
3. **数据持久化**：
   - 每个节点需要挂载本地存储卷，确保数据不会因容器重启而丢失。

在 **Spring Boot** 中使用 **Sharding-JDBC** 来实现一主一从和双主双从的配置实例，需要通过 Sharding-JDBC 的读写分离功能来实现。

------

## **1. 一主一从模式**

### **1.1 项目依赖**

在 `pom.xml` 中添加以下依赖：

```xml
<dependency>
    <groupId>org.apache.shardingsphere</groupId>
    <artifactId>shardingsphere-jdbc-core-spring-boot-starter</artifactId>
    <version>5.3.2</version> <!-- 替换为最新版本 -->
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>8.0.34</version>
</dependency>
```

------

### **1.2 配置数据源**

#### **application.yml**

以下是 Sharding-JDBC 的读写分离配置，用于一主一从模式：

```yaml
spring:
  shardingsphere:
    datasource:
      names: master, slave1 # 定义主从数据源名称

      # 主数据库
      master:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3306/master_db?useSSL=false&serverTimezone=UTC
        username: root
        password: root_password

      # 从数据库
      slave1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3307/slave_db?useSSL=false&serverTimezone=UTC
        username: root
        password: root_password

    rules:
      readwrite-splitting:
        data-sources:
          ds-rw:
            write-data-source-name: master  # 写操作使用主库
            read-data-source-names:        # 读操作使用从库
              - slave1
            load-balancer-name: round-robin
        load-balancers:
          round-robin:
            type: ROUND_ROBIN
```

------

### **1.3 测试用例**

在 Spring Boot 中创建一个简单的服务类：

#### **UserRepository.java**

```java
@Repository
public interface UserRepository extends JpaRepository<User, Long> {
}
```

#### **UserService.java**

```java
@Service
public class UserService {

    @Autowired
    private UserRepository userRepository;

    // 写操作：数据写入主库
    public void createUser(String name) {
        User user = new User();
        user.setName(name);
        userRepository.save(user);
    }

    // 读操作：数据从从库读取
    public List<User> getAllUsers() {
        return userRepository.findAll();
    }
}
```

#### **测试代码**

```java
@SpringBootTest
public class ShardingJdbcTest {

    @Autowired
    private UserService userService;

    @Test
    public void testReadWriteSplitting() {
        // 写入数据
        userService.createUser("John Doe");

        // 从从库读取数据
        List<User> users = userService.getAllUsers();
        users.forEach(user -> System.out.println(user.getName()));
    }
}
```

------

## **2. 双主双从模式**

在双主双从模式中，我们需要配置两个主数据库及其对应的从库。

### **2.1 配置数据源**

#### **application.yml**

```yaml
spring:
  shardingsphere:
    datasource:
      names: master1, slave1, master2, slave2 # 定义主从数据源名称

      # 主库1
      master1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3306/master1_db?useSSL=false&serverTimezone=UTC
        username: root
        password: root_password

      # 从库1
      slave1:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3307/slave1_db?useSSL=false&serverTimezone=UTC
        username: root
        password: root_password

      # 主库2
      master2:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3308/master2_db?useSSL=false&serverTimezone=UTC
        username: root
        password: root_password

      # 从库2
      slave2:
        type: com.zaxxer.hikari.HikariDataSource
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3309/slave2_db?useSSL=false&serverTimezone=UTC
        username: root
        password: root_password

    rules:
      readwrite-splitting:
        data-sources:
        # 读写分离类型，如: Static，Dynamic
        	type: Static
          ds-rw:
            write-data-source-name: 
              - master1
              - master2
            read-data-source-names:
              - slave1
              - slave2
            load-balancer-name: round-robin

        load-balancers:
          round-robin:
            type: ROUND_ROBIN
```

==以上如果是双主双从的情况下，Sharding-jdbc只能使用2.2中的数据分片，而不是master1与master2选一个进行插入，这也是静态类型static的配置,如果是Dynamic类型，就是Sharding-jdbc动态指定谁是主谁是从，但是需要结合zookeeper,`所以最好是一主多从`==，或者使用mycat,可以进行master1与master2选一个

------

### **2.2 数据分片策略**

在双主双从模式下，通常需要基于某种规则将请求路由到不同的主数据库。例如，可以使用分片键的范围或哈希来区分。

#### **配置分片规则**

```yaml
spring:
  shardingsphere:
    rules:
      sharding:
        tables:
          user:
            actual-data-nodes: ds-rw1.user,ds-rw2.user  # 表在两个主库中分片存储
            table-strategy:
              standard:
                sharding-column: id
                sharding-algorithm-name: user-inline

        sharding-algorithms:
          user-inline:
            type: INLINE
            props:
              algorithm-expression: ds-rw${id % 2 + 1}.user
```

------

### **2.3 测试用例**

测试与一主一从类似，Sharding-JDBC 会根据 `id` 的分片规则将数据存储到不同的主数据库中。

#### **插入测试**

```java
@Test
public void testInsert() {
    for (int i = 1; i <= 10; i++) {
        userService.createUser("User" + i);
    }
}
```

#### **验证分片结果**

使用 MySQL 客户端查询 `master1_db` 和 `master2_db` 中的 `user` 表，验证数据是否按分片规则存储。

------

## **总结**

1. **一主一从模式**：
   - 读写分离简单配置。
   - 主数据库负责写操作，从数据库负责读操作。
2. **双主双从模式**：
   - 配置多个主库和从库。
   - 配合分片规则（如哈希或范围）实现数据的分布式存储和查询。
3. **关键点**：
   - Sharding-JDBC 提供灵活的规则配置（如读写分离、分片规则）。
   - 数据库连接池配置要确保高性能和稳定性。
4. ==mysql本身也不支持分布式集群，配置比Redis复杂，而且Redis集群可以直接连接，并对数据进行分片存储，MySQL集群不可以直接连接，并不具备分片储存，需要Sharding-jdbc中间件来完成主从配置，还有数据的分片==
5. ==MySQL的数据主从复制，数据分片-》表的垂直分片（结构，并在设计阶段完成），数据库垂直分片（结构），数据库水平分片（数据），表的水平分片（数据）谨慎选择，从左到右进行选择，尽量不要同时使用==

# kafka

以下是对 **使用 Docker 配置 Kafka 集群**（含详细注释）以及通过 Spring Boot 连接 Kafka 集群并存储数据的完整步骤。

------

## **1. 使用 Docker 配置 Kafka 集群**

创建一个 `docker-compose.yml` 文件，用于启动 Kafka 集群。以下是详细配置和注释。

### **1.1 docker-compose.yml**

```yaml
version: '3.8'

services:
  # Zookeeper 服务：Kafka 的元数据管理服务
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0 # Zookeeper 镜像
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181         # Zookeeper 客户端端口
      ZOOKEEPER_TICK_TIME: 2000          # Zookeeper 的 Tick 时间
    ports:
      - "2181:2181"                      # 映射 Zookeeper 服务的端口

  # Kafka Broker 1
  kafka1:
    image: confluentinc/cp-kafka:7.3.0   # Kafka 镜像
    container_name: kafka1
    depends_on:
      - zookeeper                        # 依赖 Zookeeper 服务
    ports:
      - "9092:9092"                      # 映射 Kafka Broker 1 的端口
    environment:
      KAFKA_BROKER_ID: 1                 # Broker 的唯一 ID
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 # 连接 Zookeeper
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092 # 广播地址
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2 # offsets 的副本因子，至少 2
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT # 协议映射
      KAFKA_LOG_DIRS: /tmp/kafka-logs-1  # 数据存储目录

  # Kafka Broker 2
  kafka2:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LOG_DIRS: /tmp/kafka-logs-2

  # Kafka Broker 3
  kafka3:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka3
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_LOG_DIRS: /tmp/kafka-logs-3

networks:
  default:
    driver: bridge                       # 使用桥接网络
```

------

### **1.2 配置解释**

- **Zookeeper**:
  - Kafka 使用 Zookeeper 来存储元数据和管理 Kafka Broker 的状态。
  - `ZOOKEEPER_CLIENT_PORT` 是客户端访问 Zookeeper 的端口。
- **Kafka Broker**:
  - 每个 Broker 是集群中的一个节点，`KAFKA_BROKER_ID` 必须唯一。
  - `KAFKA_ZOOKEEPER_CONNECT` 用于连接 Zookeeper。
  - `KAFKA_ADVERTISED_LISTENERS` 定义 Broker 的外部访问地址。
  - `KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR` 设置偏移量的副本数（必须小于等于 Broker 数量）。

------

### **1.3 启动 Kafka 集群**

运行以下命令启动 Kafka 集群：

```bash
docker-compose up -d
```

------

### **1.4 创建一个 Kafka Topic**

进入 `kafka1` 容器，创建一个名为 `my-topic` 的 Topic：

```bash
docker exec -it kafka1 bash

# 创建 Topic，副本数为 2，分区数为 3
kafka-topics --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 2 --partitions 3
```

验证 Topic 创建成功：

```bash
kafka-topics --list --bootstrap-server localhost:9092
```

------

## **2. Spring Boot 连接 Kafka 集群**

接下来，我们使用 Spring Boot 连接 Kafka 集群并实现数据的生产与消费。

------

### **2.1 添加依赖**

在 `pom.xml` 中添加以下依赖：

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

------

### **2.2 配置 Kafka 连接**

在 `application.yml` 中配置 Kafka 连接信息：

```yaml
spring:
  kafka:
    bootstrap-servers:                 # Kafka 集群的 Broker 地址
      - localhost:9092
      - localhost:9093
      - localhost:9094
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: my-group               # 消费者组 ID
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      auto-offset-reset: earliest      # 如果没有消费位点，从最早开始消费
```

------

### **2.3 生产者代码**

创建一个 Kafka Producer 服务，用于向 Kafka 集群发送数据。

#### **KafkaProducerService.java**

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class KafkaProducerService {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    private static final String TOPIC = "my-topic";

    // 向 Kafka 发送消息
    public void sendMessage(String message) {
        kafkaTemplate.send(TOPIC, message);
        System.out.println("Message sent: " + message);
    }
}
```

------

### **2.4 消费者代码**

创建一个 Kafka Consumer 服务，用于消费 Kafka 集群中的数据。

#### **KafkaConsumerService.java**

```java
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class KafkaConsumerService {

    // 消费指定 Topic 的数据
    @KafkaListener(topics = "my-topic", groupId = "my-group")
    public void consume(ConsumerRecord<String, String> record) {
        System.out.println("Consumed message: " + record.value());
    }
}
```

------

### **2.5 控制器**

通过 REST 接口发送消息到 Kafka：

#### **KafkaController.java**

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class KafkaController {

    @Autowired
    private KafkaProducerService producerService;

    // 发送消息接口
    @GetMapping("/send")
    public String sendMessage(@RequestParam String message) {
        producerService.sendMessage(message);
        return "Message sent to Kafka!";
    }
}
```

------

### **2.6 测试**

1. 启动 Spring Boot 应用。

2. 通过浏览器或 Postman 调用以下接口发送消息：

   ```bash
   GET http://localhost:8080/send?message=HelloKafka
   ```

3. 查看控制台输出：

   - **生产者**：

     ```bash
     Message sent: HelloKafka
     ```

   - **消费者**：

     ```bash
     Consumed message: HelloKafka
     ```

------

## **3. Kafka 集群中数据的存储方式**

Kafka 的数据存储机制是分布式日志存储。

### **3.1 分区与副本**

1. **分区（Partition）**：
   - 一个 Topic 被分为多个分区，每个分区是一个独立的日志文件。
   - 数据写入到哪个分区由分区策略决定（默认轮询或自定义分区策略）。
2. **副本（Replica）**：
   - 每个分区可以有多个副本，其中一个为 Leader，其他为 Follower。
   - Leader 负责处理读写请求，Follower 负责同步数据。
   - 副本机制保证数据的高可用性。

------

### **3.2 数据存储路径**

Kafka Broker 会将数据存储在 `log.dirs` 指定的路径下。例如：

```yaml
KAFKA_LOG_DIRS: /tmp/kafka-logs-1
```

### **3.3 数据写入流程**

1. **生产者发送消息到 Kafka 集群**。
2. **Kafka 根据分区策略决定消息写入到哪个分区**。
3. **Leader 将数据写入本地磁盘，并通知 Follower 同步数据**。
4. **消费者根据分区读取数据**。

------

### **3.4 数据检索**

Kafka 的数据存储以 **追加写日志** 的形式存储，消息可以通过 `offset` 定位。

------

## **总结**

1. **Kafka 集群配置**：
   - 使用 Docker Compose 配置了一个包含 3 个 Broker 和 1 个 Zookeeper 的高可用 Kafka 集群。
   - 创建了一个带有分区和副本的 Topic。
2. **Spring Boot 集成 Kafka**：
   - 实现了 Kafka Producer 和 Consumer。
   - 提供 REST 接口向 Kafka Topic 发送消息，并实时消费消息。
3. **Kafka 数据存储**：
   - 数据存储在分区日志文件中，采用 **分区+副本** 的机制保证高可用性和可扩展性。

```yml
version: '3.8'

services:
  # Kafka Broker 1
  kafka1:
    image: bitnami/kafka:3.9
    container_name: kafka1
    environment:
      - KAFKA_BROKER_ID=1                          # Kafka Broker 的唯一 ID
      - KAFKA_CFG_PROCESS_ROLES=controller,broker             # Kafka 的进程角色（broker 模式）
      - KAFKA_CFG_NODE_ID=1                        # Kafka KRaft 模式下的节点 ID
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka1:9093,2@kafka2:9093,3@kafka3:9093
                                                    # 定义控制器选举集群（包括所有节点）
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
                                                    # 配置监听地址
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka1:9092
                                                    # 广播地址，供客户端连接
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
                                                    # 配置监听协议
      - KAFKA_CFG_LOG_DIRS=/bitnami/kafka/logs      # Kafka 数据存储路径
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true    # 允许自动创建 Topic
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
                                                    # offsets 的副本因子
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - ALLOW_PLAINTEXT_LISTENER=yes               # 允许明文通信（生产环境建议配置加密）
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_KRAFT_CLUSTER_ID=cluster # 所有节点必须使用相同的 ID
    ports:
      - "9092:9092"                                 # Kafka Broker 1 的外部访问端口
    volumes:
      - kafka1_data:/bitnami/kafka                 # Kafka Broker 1 的数据持久化路径
    networks:
      - kafka-network

  # Kafka Broker 2
  kafka2:
    image: bitnami/kafka:3.9
    container_name: kafka2
    environment:
      - KAFKA_BROKER_ID=2
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_NODE_ID=2
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka1:9093,2@kafka2:9093,3@kafka3:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka2:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_LOG_DIRS=/bitnami/kafka/logs
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_KRAFT_CLUSTER_ID=cluster # 所有节点必须使用相同的 ID
    ports:
      - "9093:9092"
    volumes:
      - kafka2_data:/bitnami/kafka
    networks:
      - kafka-network

  # Kafka Broker 3
  kafka3:
    image: bitnami/kafka:3.9
    container_name: kafka3
    environment:
      - KAFKA_BROKER_ID=3
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_NODE_ID=3
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka1:9093,2@kafka2:9093,3@kafka3:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka3:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_LOG_DIRS=/bitnami/kafka/logs
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_KRAFT_CLUSTER_ID=cluster # 所有节点必须使用相同的 ID
    ports:
      - "9094:9092"
    volumes:
      - kafka3_data:/bitnami/kafka
    networks:
      - kafka-network

# 数据持久化卷
volumes:
  kafka1_data:
  kafka2_data:
  kafka3_data:

# Kafka 自定义网络
networks:
  kafka-network:
    driver: bridge

```

### **1. KRaft 模式**

1. **KRaft 模式概念**：
   - Kafka 自 2.8 开始支持 KRaft 模式（Kafka 自身管理集群元数据），无需依赖 Zookeeper。
   - `CONTROLLER_QUORUM_VOTERS` 配置集群中控制器节点的地址，用于选举和协调。
2. **配置项**：
   - `KAFKA_CFG_PROCESS_ROLES=broker`：指定 Kafka 作为 Broker 节点。
   - `KAFKA_CFG_NODE_ID`：为每个节点指定唯一的节点 ID。
   - `KAFKA_CFG_CONTROLLER_QUORUM_VOTERS`：定义控制器选举的节点信息（格式为 `<node_id>@<hostname>:<controller-port>`）。

------

### **2. 持久化**

1. 存储路径

   ：

   - 每个 Kafka Broker 的数据存储在 `/bitnami/kafka` 内，持久化到宿主机磁盘。

   - 例如：

     ```
     yaml
     
     
     复制代码
     volumes:
       kafka1_data:/bitnami/kafka
     ```

2. 为什么需要持久化

   ：

   - 确保 Kafka Broker 重启后，数据不会丢失。

------

### **3. 监听器配置**

1. **`KAFKA_CFG_LISTENERS`**：
   - 定义 Broker 的监听地址：
     - `PLAINTEXT`：客户端访问的普通通信端口（如 `9092`）。
     - `CONTROLLER`：Broker 间通信的控制端口（如 `9093`）。
2. **`KAFKA_CFG_ADVERTISED_LISTENERS`**：
   - 定义客户端可以访问的地址（如 `PLAINTEXT://kafka1:9092`）。
3. **`KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP`**：
   - 映射每种监听器的通信协议，这里都为明文通信（生产环境建议配置加密）。

------

### **4. 副本和高可用配置**

1. **副本因子**：
   - `KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3`：`__consumer_offsets` topic 的副本因子。
   - `KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3`：事务日志的副本因子。
2. **最小同步副本数**：
   - `KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2`：事务日志的最小同步副本数。
3. ==kafka本身具有分布式集群的效果，与elasticsearch一样，都有分区，副本的概念==



# Rabbitmq

以下是如何使用 Docker 和 Docker Compose 创建一个 RabbitMQ 集群的详细步骤。

------

### **RabbitMQ 集群介绍**

- RabbitMQ 集群是由多个 RabbitMQ 节点组成，用于实现高可用性和负载均衡。
- 每个节点可以是主节点或从节点，使用 Erlang cookie 作为节点之间通信的安全令牌。
- 数据可以使用镜像队列在多个节点之间同步。

------

### **创建 RabbitMQ 集群的步骤**

#### **1. 准备 `docker-compose.yml` 文件**

创建一个 `docker-compose.yml` 文件如下：

```yaml
version: '3.8'

services:
  # RabbitMQ Node 1
  rabbitmq1:
    image: rabbitmq:3.11-management   # 带有管理插件的 RabbitMQ 镜像
    container_name: rabbitmq1
    hostname: rabbitmq1               # RabbitMQ 节点的主机名
    environment:
      RABBITMQ_ERLANG_COOKIE: "mysecretcookie"  # Erlang cookie，用于集群通信，所有节点必须一致
      RABBITMQ_DEFAULT_USER: admin             # 默认用户
      RABBITMQ_DEFAULT_PASS: admin             # 默认密码
    ports:
      - "15672:15672"                # 管理界面端口
      - "5672:5672"                  # 应用连接端口
    volumes:
      - rabbitmq1_data:/var/lib/rabbitmq  # 数据持久化
    networks:
      - rabbitmq_network

  # RabbitMQ Node 2
  rabbitmq2:
    image: rabbitmq:3.11-management
    container_name: rabbitmq2
    hostname: rabbitmq2
    environment:
      RABBITMQ_ERLANG_COOKIE: "mysecretcookie"
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin
    ports:
      - "15673:15672"
      - "5673:5672"
    volumes:
      - rabbitmq2_data:/var/lib/rabbitmq
    networks:
      - rabbitmq_network
    depends_on:
      - rabbitmq1

  # RabbitMQ Node 3
  rabbitmq3:
    image: rabbitmq:3.11-management
    container_name: rabbitmq3
    hostname: rabbitmq3
    environment:
      RABBITMQ_ERLANG_COOKIE: "mysecretcookie"
      RABBITMQ_DEFAULT_USER: admin
      RABBITMQ_DEFAULT_PASS: admin
    ports:
      - "15674:15672"
      - "5674:5672"
    volumes:
      - rabbitmq3_data:/var/lib/rabbitmq
    networks:
      - rabbitmq_network
    depends_on:
      - rabbitmq1
      - rabbitmq2

# 定义持久化卷
volumes:
  rabbitmq1_data:
  rabbitmq2_data:
  rabbitmq3_data:

# 自定义网络
networks:
  rabbitmq_network:
    driver: bridge
```

------

#### **2. 启动 RabbitMQ 集群**

运行以下命令启动 RabbitMQ 容器：

```bash
docker-compose up -d
```

------

#### **3. 配置 RabbitMQ 集群**

RabbitMQ 集群不会自动形成，我们需要手动将节点加入到集群中。

##### **步骤 1：进入第 2 个和第 3 个节点容器**

进入 RabbitMQ 节点 2 和节点 3 容器：

```bash
docker exec -it rabbitmq2 bash
docker exec -it rabbitmq3 bash
```

##### **步骤 2：将节点加入集群**

在 `rabbitmq2` 容器中，运行以下命令将节点 2 加入到节点 1 的集群：

```bash
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@rabbitmq1
rabbitmqctl start_app
```

在 `rabbitmq3` 容器中，运行以下命令将节点 3 加入到节点 1 的集群：

```bash
rabbitmqctl stop_app
rabbitmqctl join_cluster rabbit@rabbitmq1
rabbitmqctl start_app
```

------

#### **4. 验证 RabbitMQ 集群**

1. **访问 RabbitMQ 管理界面**：

   - RabbitMQ 提供了一个管理界面，你可以通过以下地址访问每个节点：
     - 节点 1: [http://localhost:15672](http://localhost:15672/)
     - 节点 2: [http://localhost:15673](http://localhost:15673/)
     - 节点 3: [http://localhost:15674](http://localhost:15674/)

   使用默认用户名和密码登录：

   - 用户名：`admin`
   - 密码：`admin`

2. **查看集群状态**：

   - 登录 RabbitMQ 管理界面后，导航到 **"Cluster"** 页面，应该可以看到 3 个节点已经连接在一起。

------

#### **5. 配置镜像队列**

镜像队列是 RabbitMQ 的一种高级功能，可以在多个节点之间复制队列内容，以提高高可用性。

在 RabbitMQ 管理界面中，按照以下步骤配置镜像队列：

1. 登录管理界面，进入 **"Policies"** 标签。
2. 创建一个新的策略：
   - **Name**: `ha-all`
   - **Pattern**: `.*` （匹配所有队列）
   - **Definition**: `{"ha-mode":"all"}` （将队列镜像到所有节点）
   - **Priority**: `0`
3. 保存策略。

------

### **测试 RabbitMQ 集群**

1. **发送消息到队列**： 使用任何 RabbitMQ 客户端（例如 `pika` 或 `RabbitMQ CLI`）向集群发送消息。

2. **关闭任意节点**： 使用以下命令停止任意节点：

   ```bash
   docker stop rabbitmq2
   ```

   然后验证消息是否依然可以通过其他节点消费。

------

### **总结**

- 使用 Docker Compose 创建了一个包含 3 个节点的 RabbitMQ 集群。
- 使用 Erlang cookie 配置了节点间通信安全性。
- 通过手动命令将节点加入集群。
- 配置了镜像队列以实现高可用性。

此设置可以适用于测试和生产环境，生产环境中建议添加更多安全配置（如 TLS 和用户权限控制）。

==rabbitmq与MySQL集群类似没有数据分区的概念，本身都不具备分布式集群，需要手动添加到集群，还有要设置镜像队列，但是又不需要中间件==

以下是 Spring Boot 连接 RabbitMQ 集群并进行测试的完整配置和步骤。

------

## **1. 添加 Spring Boot 依赖**

在 `pom.xml` 文件中添加 RabbitMQ 相关依赖：

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-amqp</artifactId>
</dependency>
```

------

## **2. 配置 Spring Boot 连接 RabbitMQ 集群**

在 `application.yml` 文件中配置 RabbitMQ 集群的连接信息。以下是一个典型的配置：

```yaml
spring:
  rabbitmq:
    addresses: amqp://rabbitmq1:5672,amqp://rabbitmq2:5673,amqp://rabbitmq3:5674 # RabbitMQ 集群地址
    username: admin              # 登录用户名
    password: admin              # 登录密码
    virtual-host: /              # 默认虚拟主机
```

**说明**：

- `addresses`

  ：

  - RabbitMQ 集群节点的地址，使用逗号分隔多个节点。
  - 在 Docker Compose 中，我们使用 `rabbitmq1`、`rabbitmq2`、`rabbitmq3` 作为容器的主机名。

- `username` 和 `password`

  ：

  - RabbitMQ 的登录凭据，与 Docker Compose 文件中配置的 `RABBITMQ_DEFAULT_USER` 和 `RABBITMQ_DEFAULT_PASS` 保持一致。

------

## **3. 创建消息生产者和消费者**

### **生产者**

创建一个生产者服务，用于发送消息到 RabbitMQ。

#### **ProducerService.java**

```java
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

@Service
public class ProducerService {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    private static final String EXCHANGE_NAME = "test-exchange";
    private static final String ROUTING_KEY = "test-routing-key";

    public void sendMessage(String message) {
        rabbitTemplate.convertAndSend(EXCHANGE_NAME, ROUTING_KEY, message);
        System.out.println("Message sent: " + message);
    }
}
```

------

### **消费者**

创建一个消费者，用于接收来自 RabbitMQ 的消息。

#### **ConsumerService.java**

```java
import org.springframework.amqp.rabbit.annotation.RabbitListener;
import org.springframework.stereotype.Service;

@Service
public class ConsumerService {

    @RabbitListener(queues = "test-queue")
    public void receiveMessage(String message) {
        System.out.println("Message received: " + message);
    }
}
```

------

### **消息队列配置**

创建 RabbitMQ 的 Exchange、Queue 和绑定。

#### **RabbitConfig.java**

```java
import org.springframework.amqp.core.Binding;
import org.springframework.amqp.core.BindingBuilder;
import org.springframework.amqp.core.Queue;
import org.springframework.amqp.core.TopicExchange;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class RabbitConfig {

    private static final String QUEUE_NAME = "test-queue";
    private static final String EXCHANGE_NAME = "test-exchange";
    private static final String ROUTING_KEY = "test-routing-key";

    // 定义队列
    @Bean
    public Queue queue() {
        return new Queue(QUEUE_NAME, true); // true 表示队列持久化
    }

    // 定义交换机
    @Bean
    public TopicExchange exchange() {
        return new TopicExchange(EXCHANGE_NAME);
    }

    // 定义绑定关系
    @Bean
    public Binding binding(Queue queue, TopicExchange exchange) {
        return BindingBuilder.bind(queue).to(exchange).with(ROUTING_KEY);
    }
}
```

------

## **4. 测试 RabbitMQ 集群**

### **创建测试接口**

创建一个 REST 接口，用于调用生产者服务发送消息。

#### **RabbitController.java**

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class RabbitController {

    @Autowired
    private ProducerService producerService;

    @GetMapping("/send")
    public String sendMessage(@RequestParam String message) {
        producerService.sendMessage(message);
        return "Message sent to RabbitMQ: " + message;
    }
}
```

------

### **启动 Spring Boot 应用**

启动 Spring Boot 应用程序，确保连接到 RabbitMQ 集群。

------

### **测试步骤**

1. **发送消息**：

   - 通过浏览器或 Postman 调用以下接口：

     ```bash
     GET http://localhost:8080/send?message=HelloRabbitMQ
     ```

2. **验证消费者是否接收到消息**：

   - 在 Spring Boot 应用的日志中，应该能看到类似以下的输出：

     ```text
     Message sent: HelloRabbitMQ
     Message received: HelloRabbitMQ
     ```

3. **验证集群高可用性**：

   - 停止其中一个 RabbitMQ 节点，例如：

     ```bash
     docker stop rabbitmq2
     ```

   - 再次调用 `/send` 接口，验证消息是否正常发送并接收。

------

## **集群高可用测试**

1. **故障切换**：
   - 停止或重启 RabbitMQ 集群中的某个节点，验证消息是否能够通过其他节点正常路由。
2. **镜像队列验证**：
   - 在 RabbitMQ 管理界面中，查看 `test-queue` 是否被镜像到所有节点。

------

## **总结**

1. **Spring Boot 配置**：
   - 通过 `spring.rabbitmq.addresses` 配置连接 RabbitMQ 集群。
   - 使用生产者和消费者模式进行消息的发送和接收。
2. **RabbitMQ 集群**：
   - 使用 Docker 创建了一个包含 3 个节点的高可用 RabbitMQ 集群。
   - 配置了镜像队列，确保消息在集群中同步。
3. **高可用性测试**：
   - 停止某个节点，验证集群的容错能力。

此配置可以满足生产环境下对 RabbitMQ 高可用和负载均衡的需求。